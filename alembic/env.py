import sys
sys.path.append('/home/tapis/service')

import logging
from logging.config import fileConfig
import re
import os

from sqlalchemy import create_engine
from sqlalchemy import pool

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)
logger = logging.getLogger("alembic.env")


### Service Changes. We pull info from the main code. Like sites and tenants so we can add that data to the alembic.ini file.
from tapisservice.config import conf
from sqlmodel import create_engine, Session, select, SQLModel
from stores import pg_store, pg_default

# Get config settings to create conninfo later
username = conf.postgres_user
password = conf.postgres_pass
host = conf.postgres_host

engines = {}
all_urls = {}
# Create databases and schemas wanted.
for site, tenants in pg_store.items():
    # Create database and fail gracefully if it already exists
    try:
        pg_default.run('execute', f'CREATE DATABASE "{site}"', autocommit=True)
    except:
        msg = f"Database for site: {site}, already exists. Skipping."
        logger.warning(msg)
    
    # Create schemas for each tenant
    for tenant, pg in tenants.items():
        try:
            # TODO indexes! #CREATE CONSTRAINT FOR (p:Pod) REQUIRE p.name IS UNIQUE
            pg.run("execute", f'CREATE SCHEMA IF NOT EXISTS "{tenant}"', autocommit=True)
        except Exception as e:
            msg = f"Error when creating schemas for tenant: {tenant}. e: {repr(e)}"
            logger.warning(msg)

        # Add database connection info to alembic config
        conninfo = f"postgresql://{username}:{password}@{host}/{site}"#?options=-csearch_path%3Ddbo,{tenant}"
        name = f"{site}_{tenant}".replace("-", "HYPHEN") # Some tenants have - in their names. This messes up alembic later.
        all_urls[site] = conninfo
        engines[name] = create_engine(conninfo, future=False)
        config.set_section_option(name, "sqlalchemy.url", conninfo)

# Add databases to alembic's list of databases.
config.set_main_option("databases", ",".join(all_urls.keys()))

# gather section names referring to databases.
db_names = config.get_main_option("databases")
logger.warning(f"Using the following databases with alembic: {db_names}")

######### Import all of the models we want to be autogenerated. Will proliferate to all schemas.
from models import ExportedData, Pod, Password
target_metadata = SQLModel.metadata

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_example_option = config.get_main_option("my_example_option")

def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    # Create transactions/connections here so we can rollback/close later.
    connections = []
    transactions = []
    logger.info(f"engine: {engines}")

    try:
        # Create tables in each tenant(schema) in each database.
        for name, engine in engines.items():
            site, tenant = name.split('_')
            tenant = tenant.replace('HYPHEN', '-')

            ## GO THROUGH ALL TENANTS, MEANING MIGRATE TO EACH SCHEMA!
            logger.info(f"Migrating database {site}; tenant {tenant}")
            # Reference https://alembic.sqlalchemy.org/en/latest/cookbook.html#rudimental-schema-level-multi-tenancy-for-postgresql-databases
            # Get engine/connection/transaction and add some schema stuff to it.
            logger.info(f"{engine}")
            connection = engine.connect()
            logger.info(f"{connection}")
            connection.execute(f'set search_path to "{tenant}"')
            connection.dialect.default_schema_name = tenant
            connections.append(connection)
            # Create transaction
            transaction = connection.begin()
            transactions.append(transaction)
            # Create context
            context.configure(
                connection=connection,
                include_schemas=True,
                upgrade_token=f"upgrade_alltenants",
                downgrade_token=f"downgrade_alltenants",
                target_metadata=target_metadata,
            )
            context.run_migrations(engine_name=name)

        for transaction in transactions:
            transaction.commit()
    except:
        for transaction in transactions:
            transaction.rollback()
        raise
    finally:
        for connection in connections:
            connection.close()


run_migrations_online()
